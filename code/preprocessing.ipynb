{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't use standard preprocessing steps like stemming or stopword removal when you have pre-trained embeddings\n",
    "#Some of you might used standard preprocessing steps when doing word count based feature extraction \n",
    "#(e.g. TFIDF) such as removing stopwords, stemming etc. \n",
    "#The reason is simple: You loose valuable information, which would help your NN to figure things out.\n",
    "\n",
    "#Get your vocabulary as close to the embeddings as possible\n",
    "#I will focus in this notebook, how to achieve that. \n",
    "#For an example I take the GoogleNews pretrained embeddings, \n",
    "#there is no deeper reason for this choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 40589: expected 3 fields, saw 88109\\nSkipping line 81028: expected 3 fields, saw 88235\\nSkipping line 121424: expected 3 fields, saw 88056\\nSkipping line 161735: expected 3 fields, saw 87702\\nSkipping line 202162: expected 3 fields, saw 87833\\nSkipping line 242604: expected 3 fields, saw 88078\\n'\n",
      "b'Skipping line 278263: expected 3 fields, saw 77398\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (1002366, 3)\n",
      "Test Shape: (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv',error_bad_lines=False)\n",
    "test  = pd.read_csv('test.csv',error_bad_lines=False)\n",
    "print(f'Train Shape: {train.shape}')\n",
    "print(f'Test Shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1002366/1002366 [00:04<00:00, 248688.23it/s]\n",
      "100%|██████████| 1002366/1002366 [00:03<00:00, 278889.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': 201006, 'did': 25570, 'Quebec': 76, 'nationalists': 70, 'see': 6900}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "news_path = 'GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430209/430209 [00:01<00:00, 269333.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 25.95% of vocab\n",
      "Found embeddings for  78.75% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 309623),\n",
       " ('a', 309121),\n",
       " ('of', 253817),\n",
       " ('and', 193805),\n",
       " ('India?', 12561),\n",
       " ('it?', 9900),\n",
       " ('do?', 6835),\n",
       " ('life?', 5996),\n",
       " ('you?', 4800),\n",
       " ('them?', 4782),\n",
       " ('me?', 4771),\n",
       " ('time?', 4317),\n",
       " ('world?', 4193),\n",
       " ('people?', 3768),\n",
       " ('why?', 3730),\n",
       " ('Quora?', 3589),\n",
       " ('10', 3502),\n",
       " ('like?', 3457),\n",
       " ('for?', 3383),\n",
       " ('work?', 3234)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(oov) words that we can use to improve our preprocessing\n",
    "oov = check_coverage(vocab,embeddings_index)\n",
    "oov[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 :remove symbol\n",
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1002366/1002366 [00:08<00:00, 112185.25it/s]\n",
      "100%|██████████| 1002366/1002366 [00:03<00:00, 279073.05it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 218035/218035 [00:00<00:00, 293197.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 60.36% of vocab\n",
      "Found embeddings for  89.99% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 312050),\n",
       " ('a', 310015),\n",
       " ('of', 255475),\n",
       " ('and', 195395),\n",
       " ('2017', 6756),\n",
       " ('2018', 5652),\n",
       " ('10', 5072),\n",
       " ('12', 2869),\n",
       " ('20', 2277),\n",
       " ('100', 2201),\n",
       " ('15', 2131),\n",
       " ('12th', 1954),\n",
       " ('11', 1784),\n",
       " ('30', 1687),\n",
       " ('18', 1636),\n",
       " ('50', 1498),\n",
       " ('16', 1221),\n",
       " ('14', 1187),\n",
       " ('17', 1156),\n",
       " ('13', 1062)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)\n",
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2 : remove number\n",
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1002366/1002366 [00:11<00:00, 88386.30it/s]\n",
      "100%|██████████| 1002366/1002366 [00:03<00:00, 261707.73it/s]\n",
      "100%|██████████| 1002366/1002366 [00:03<00:00, 292019.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209295/209295 [00:00<00:00, 272354.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 63.42% of vocab\n",
      "Found embeddings for  90.75% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 312050),\n",
       " ('a', 310015),\n",
       " ('of', 255475),\n",
       " ('and', 195395),\n",
       " ('favourite', 959),\n",
       " ('colour', 763),\n",
       " ('bitcoin', 756),\n",
       " ('doesnt', 711),\n",
       " ('centre', 684),\n",
       " ('Quorans', 654),\n",
       " ('cryptocurrency', 637),\n",
       " ('Snapchat', 605),\n",
       " ('travelling', 522),\n",
       " ('counselling', 487),\n",
       " ('btech', 482),\n",
       " ('didnt', 471),\n",
       " ('cryptocurrencies', 388),\n",
       " ('Brexit', 381),\n",
       " ('behaviour', 357),\n",
       " ('blockchain', 356)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)\n",
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3 : build a dict to do simple preprocessing remove prep and mispell and top underflow\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'bitcoins':'bitcoin',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'socialmedium',\n",
    "                'whatsapp': 'socialmedium',\n",
    "                'Snapchat': 'socialmedium',\n",
    "                'Btech': 'btech',\n",
    "                \n",
    "                \n",
    "\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)  #replace all the keys if in dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1002366/1002366 [00:04<00:00, 209287.47it/s]\n",
      "100%|██████████| 1002366/1002366 [00:03<00:00, 263695.64it/s]\n",
      "100%|██████████| 1002366/1002366 [00:04<00:00, 216865.27it/s]\n",
      "100%|██████████| 1002366/1002366 [00:03<00:00, 307162.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','an','to','of','and','is','it','that','am','are','were']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209248/209248 [00:00<00:00, 251310.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 63.43% of vocab\n",
      "Found embeddings for  98.89% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bitcoin', 949),\n",
       " ('colour', 763),\n",
       " ('btech', 675),\n",
       " ('Quorans', 654),\n",
       " ('cryptocurrency', 637),\n",
       " ('socialmedium', 614),\n",
       " ('cryptocurrencies', 388),\n",
       " ('Brexit', 381),\n",
       " ('behaviour', 357),\n",
       " ('blockchain', 356),\n",
       " ('upvotes', 325),\n",
       " ('programme', 317),\n",
       " ('Redmi', 299),\n",
       " ('realise', 289),\n",
       " ('defence', 276),\n",
       " ('KVPY', 273),\n",
       " ('Paytm', 256),\n",
       " ('grey', 232),\n",
       " ('mtech', 208),\n",
       " ('upvote', 196)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)\n",
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
